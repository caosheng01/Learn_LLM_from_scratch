# 再谈微调技术：LoRA

## 背景

我们在基础篇《大语言模型：通往通用人工智能之路》中提到**两阶段训练法**， 即**预训练**（Pre-Train)和**微调**(Fine Tune)。
在本章中，我们讲详细聊聊其中的SFT部分。而站在2024年的时间节点上，其中最重要的技术就是LoRA(Low-Rank Adaptation), 故本文将着重讲述LoRA技术。

## 提出问题

微调的含义，就是把已经训练好的模型（PTM, Pre-Train Model)拿来，用特定的下游任务数据来继续训练这个PTM，使得模型在预训练权重上继续更新权重，直至满足下游任务性能标准。

### 全量参数微调

**全量参数微调**指的是，在下游任务的训练中，对预训练模型的每一个参数都做更新。

而随着大语言模型的越来越大，全量参数微调的成本也越来越昂贵了。以ChatGPT3.5 175B 这种级别的LLM为例，世界上，没有几家公司能够支付的起日常的训练费用，一次训练费用需要百万美元以上。哪怕是小两个数量级，我们拿6B的LLM为例，全量参数微调也是非常昂贵的。
2024年，单机8卡（入门级别的显卡A6000）为例。一次全量参数训练，需要近1000小时。而一个含有LLM的软件到达产品发布级别的质量，要经过十轮以上的微调。除此之外，模型全量微调还会损失多样性，存在灾难性遗忘的问题。因此，在2024年，业界已经越来越少的采用全量参数微调了。

因此，我们就提出要解决的问题——如何高效的进行模型微调。

## 解决问题

如何高效的进行模型微调，成了业界研究的重点，发展出了很多种办法，这些方法统称**参数高效微调**（PEFT, Parameter-Efficient Fine-Tuning）。

### 参数高效微调

参数高效微调是指微调少量或额外的模型参数，固定大部分预训练模型(LLM)参数，从而大大降低了计算和存储成本，同时，也能实现与全量参数微调相当的性能，可以更好地泛化到域外场景。

高效微调技术可以粗略分为以下三大类：增加额外参数(Additive), 选取一部分参数更新(Selective)、引入重参数化(Reparametrization-based)。而在增加额外参数这类方法中，又主要分为类适配器(Adapter-like)方法和软提示(Soft prompts)两个小类。
下图中，把各种PEFT方法按照上述分类做了一个总结。
![LoRA_PEFT.png](../images/LoRA_PEFT.png)

目前，从效果来讲，增加额外参数(Additive)效果最好，LoRA也属于这一类中，如果从发展史来介绍各种代表性的算法，文章要写的很长。本文的重点是LoRA，所以笔者就介绍LoRA原论文里提及的两种算法，即Adapter-tuning和Prefix-tuning。

### Adapter-tuning

### Prefix-tuning

## LoRA

### PEFT想要解决的问题

在介绍LoRA之前，先简单总结一下上面提到的几个解决办法。

* **全参数微调**太贵;
* **Adapter Tuning**存在训练和推理延迟，
* **Prefix Tuning**难训且会减少原始训练数据中的有效文字长度​.

那是否有一种微调办法，能改善这些不足呢？我们不妨回到问题本身，

> 所谓的参数高效微调(PEFT)本质上就是通过训练少数非常重要的参数，达到或者接近全参数微调的效果。

那么问题又来了，上面提到了一个非常口语化的描述——“少数非常重要的参数”。又引入了两个问题：

1. 问题一：到底用多少来表示“少数”？
2. 问题二：什么才是“重要”的？

现在我们来尝试解答前面提到的两个问题。
问题一呢，相对比较好解答，没有太多的理论推导。按照成本考虑的话，一般要做到减少2个数量级才会有比较好的性价比。这也是工程界经常用近似值来模拟精确的值来达到简化计算的结果。

#### 一个简单的例子

求：$\sqrt{901,800,900}$的值。

$$
\begin{aligned}

& \because 901,800,900 \approx 900,000,000 \\
& \therefore \sqrt{901,800,900} \approx \sqrt{900,000,000} = 30,000

\end{aligned}
$$

上述这个例子就是直接用近似值$900,000,000$来代替精确值$901,800,900$，直接口算出这个题的近似答案。

现在我们来思考一下

> 问题二：什么才是“重要”的？

现在我们回到刚才这个简单的例子，我们尝试求解一下$\sqrt{901,800,900}$的精确值。参加过九年制义务教育的我们，很快就能找到求解方法——因式分解

$$
\sqrt{901,800,900} = \sqrt{(2 \times 3 \times 5 \times 7 \times 11 \times 13)^2} = 30,030
$$

我们仔细思考一下**因式分解**过程，其实这个方法就是找到对$901,800,900$而言，重要的几个数：$2,3,5,7,11,13$，即**质数**(Prime Number)。随便说一下，Prime在英语中，其实就有“重要的”这个意思。
Tips: 上述这个例子过于简单，大家借鉴一下思路。如果想深入理解如何找到“重要”的参数思想，笔者建议去学一下经典的PCA(Principal Component Analysis)算法。

基础篇中，笔者已经说明，机器学习其实就是调整矩阵参数的权重。结合一下上面这个例子，我们能不能找到类似于的办法解决。
问题就转变为，如何找到矩阵的“少数且重要的参数”。我们不禁会问，初等数学中的因式分解和质数的概念，在矩阵中有没有类似的概念呢？答案是，有的。矩阵的**秩**(Rank)和**奇异值分解**(SVD,Singular Value Decomposition)就是对应**质数**和**因式分解**的概念。

### 秩

先给出秩的定义

> 对于一个$m×n$矩阵$A$，其秩定义为A中最高阶非零子式的阶数，记作$R(A)=r$或$rank(A)=r$。这里，非零子式指的是从矩阵$A$中任取$k$行与$k$列$（k≤min(m,n)）$所构成的k阶行列式，且该行列式不为零。

再看几个例子，体会一下，什么是秩。

$$
A = \left[\begin{matrix}1 & 2 & 3 \\ 2 & 4 & 6 \\ 3 & 6 & 9  \end{matrix}\right]; 
B = \left[\begin{matrix}1 & 2 & 3 \\ 0 & 1 & 4 \\ 1 & 3 & 7  \end{matrix}\right];
C = \left[\begin{matrix}1 & 0 & 2 \\ 0 & 1 & 3 \\ 4 & 5 & 0  \end{matrix}\right];
$$

矩阵A，每一行都是第一行的倍数，因此，$Rank(A)=1$。
矩阵B，第一行和第二行是线性无关的，而第三行可以表示为第一行和第二行的线性组合,即$Row3=Row1+Row2$, 因此，$Rank(B)=2$。
矩阵C，没有一行（或一列）是其他行（或列）的线性组合。因此，$Rank(B)=3$。

### 奇异值分解

奇异值分解(SVD)是矩阵分解的一种形式，具有明确的数学定义。以下是对奇异值分解的数学定义：

奇异值分解是指将一个秩为r的实矩阵A（大小为m×n）分解为三个实矩阵乘积的形式：

$$
A = U \Sigma V^T
$$

其中：

* $U$是一个$m×m$的正交矩阵，其列向量称为左奇异向量。
* $V$是一个$n×n$的正交矩阵，其列向量称为右奇异向量。
* $\Sigma$是一个$m×n$的矩形对角矩阵（或称为对角块矩阵），称为奇异值矩阵。其对角线上的元素（或对角块）称为奇异值，且这些奇异值是非负的，并按照降序排列（尽管在数学定义中不强制要求降序排列，但在实际应用中通常会这样做以便于分析和处理）。

### 矩阵降维

介绍完秩和SVD之后，我们来继续讨论一下我们关心的问题，如何把一个庞大的矩阵，变成一个较小的矩阵，来达到简化运算的目的。这个思路就类似于，上面那个初级数学中找个接近的近似数来代替精确值的办法。
我们把这个问题，用具体的例子来表述一下。

$$
A = \left[\begin{matrix}1 & 0 & \cdots & 2 \\ 0 & 1 & \cdots & 3 \\ \vdots & \vdots & \ddots & \vdots \\ 4 & 5 & \cdots & 0  \end{matrix}\right]_{90,000 \times 1,000} 
\longrightarrow

A' = \left[\begin{matrix}1 & 0 & \cdots & 2 \\ 0 & 1 & \cdots & 3 \\ \vdots & \vdots & \ddots & \vdots \\ 4 & 5 & \cdots & 0  \end{matrix}\right]_{900 \times 10}
$$

我们需要用一个算法，找到A'，然后用A‘来代替A，从而极大的降低计算成本。这个过程，用洋气且专业的名字叫“降维”，即把矩阵的行和列，压缩一下（2个数量级及以上）。



## 参考文献

1. [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647)

