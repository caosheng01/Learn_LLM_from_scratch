# 再谈微调技术：LoRA

## 背景

我们在基础篇《大语言模型：通往通用人工智能之路》中提到**两阶段训练法**， 即**预训练**（Pre-Train)和**微调**(Fine Tune)。
在本章中，我们讲详细聊聊其中的SFT部分。而站在2024年的时间节点上，其中最重要的技术就是LoRA(Low-Rank Adaptation), 故本文将着重讲述LoRA技术。

## 提出问题

微调的含义，就是把已经训练好的模型（PTM, Pre-Train Model)拿来，用特定的下游任务数据来继续训练这个PTM，使得模型在预训练权重上继续更新权重，直至满足下游任务性能标准。

### 全量参数微调

**全量参数微调**指的是，在下游任务的训练中，对预训练模型的每一个参数都做更新。

而随着大语言模型的越来越大，全量参数微调的成本也越来越昂贵了。以ChatGPT3.5 175B 这种级别的LLM为例，世界上，没有几家公司能够支付的起日常的训练费用，一次训练费用需要百万美元以上。哪怕是小两个数量级，我们拿6B的LLM为例，全量参数微调也是非常昂贵的。
2024年，单机8卡（入门级别的显卡A6000）为例。一次全量参数训练，需要近1000小时。而一个含有LLM的软件到达产品发布级别的质量，要经过十轮以上的微调。除此之外，模型全量微调还会损失多样性，存在灾难性遗忘的问题。因此，在2024年，业界已经越来越少的采用全量参数微调了。

因此，我们就提出要解决的问题——如何高效的进行模型微调。

## 解决问题

如何高效的进行模型微调，成了业界研究的重点，发展出了很多种办法，这些方法统称**参数高效微调**（PEFT, Parameter-Efficient Fine-Tuning）。

### 参数高效微调

参数高效微调是指微调少量或额外的模型参数，固定大部分预训练模型(LLM)参数，从而大大降低了计算和存储成本，同时，也能实现与全量参数微调相当的性能，可以更好地泛化到域外场景。

高效微调技术可以粗略分为以下三大类：增加额外参数(Additive), 选取一部分参数更新(Selective)、引入重参数化(Reparametrization-based)。而在增加额外参数这类方法中，又主要分为类适配器(Adapter-like)方法和软提示(Soft prompts)两个小类。
下图中，把各种PEFT方法按照上述分类做了一个总结。
![LoRA_PEFT.png](../images/LoRA_PEFT.png)

目前，从效果来讲，增加额外参数(Additive)效果最好，LoRA也属于这一类中，如果从发展史来介绍各种代表性的算法，文章要写的很长。本文的重点是LoRA，所以笔者就介绍LoRA原论文里提及的两种算法，即Adapter-tuning和Prefix-tuning。

### Adapter-tuning

### Prefix-tuning

## LoRA低秩适配

### PEFT想要解决的问题

在介绍LoRA之前，先简单总结一下上面提到的几个解决办法。

* **全参数微调**太贵;
* **Adapter Tuning**存在训练和推理延迟，
* **Prefix Tuning**难训且会减少原始训练数据中的有效文字长度​.

那是否有一种微调办法，能改善这些不足呢？我们不妨回到问题本身，

> 所谓的参数高效微调(PEFT)本质上就是通过训练少数非常重要的参数，达到或者接近全参数微调的效果。

那么问题又来了，上面提到了一个非常口语化的描述——“少数非常重要的参数”。又引入了两个问题：

1. 问题一：到底用多少来表示“少数”？
2. 问题二：什么才是“重要”的？

现在我们来尝试解答前面提到的两个问题。
问题一呢，相对比较好解答，没有太多的理论推导。按照成本考虑的话，一般要做到减少2个数量级才会有比较好的性价比。这也是工程界经常用近似值来模拟精确的值来达到简化计算的结果。

#### 一个简单的例子

求：$\sqrt{901,800,900}$的值。

$$
\begin{aligned}

& \because 901,800,900 \approx 900,000,000 \\
& \therefore \sqrt{901,800,900} \approx \sqrt{900,000,000} = 30,000

\end{aligned}
$$

上述这个例子就是直接用近似值$900,000,000$来代替精确值$901,800,900$，直接口算出这个题的近似答案。

现在我们来思考一下

> 问题二：什么才是“重要”的？

现在我们回到刚才这个简单的例子，我们尝试求解一下$\sqrt{901,800,900}$的精确值。参加过九年制义务教育的我们，很快就能找到求解方法——因式分解

$$
\sqrt{901,800,900} = \sqrt{(2 \times 3 \times 5 \times 7 \times 11 \times 13)^2} = 30,030
$$

我们仔细思考一下**因式分解**过程，其实这个方法就是找到对$901,800,900$而言，重要的几个数：$2,3,5,7,11,13$，即**质数**(Prime Number)。随便说一下，Prime在英语中，其实就有“重要的”这个意思。
Tips: 上述这个例子过于简单，大家借鉴一下思路。如果想深入理解如何找到“重要”的参数思想，笔者建议去学一下经典的PCA(Principal Component Analysis)算法。

基础篇中，笔者已经说明，机器学习其实就是调整矩阵参数的权重。结合一下上面这个例子，我们能不能找到类似于的办法解决。
问题就转变为，如何找到矩阵的“少数且重要的参数”。我们不禁会问，初等数学中的因式分解和质数的概念，在矩阵中有没有类似的概念呢？答案是，有的。矩阵的**秩**(Rank)和**奇异值分解**(SVD,Singular Value Decomposition)就是对应**质数**和**因式分解**的概念。

### 秩

先给出秩的定义

> 对于一个$m×n$矩阵$A$，其秩定义为A中最高阶非零子式的阶数，记作$R(A)=r$或$rank(A)=r$。这里，非零子式指的是从矩阵$A$中任取$k$行与$k$列$（k≤min(m,n)）$所构成的k阶行列式，且该行列式不为零。

再看几个例子，体会一下，什么是秩。

$$
A = \left[\begin{matrix}1 & 2 & 3 \\ 2 & 4 & 6 \\ 3 & 6 & 9  \end{matrix}\right]; 
B = \left[\begin{matrix}1 & 2 & 3 \\ 0 & 1 & 4 \\ 1 & 3 & 7  \end{matrix}\right];
C = \left[\begin{matrix}1 & 0 & 2 \\ 0 & 1 & 3 \\ 4 & 5 & 0  \end{matrix}\right];
$$

矩阵A，每一行都是第一行的倍数，因此，$rank(A)=1$。
矩阵B，第一行和第二行是线性无关的，而第三行可以表示为第一行和第二行的线性组合,即$Row3=Row1+Row2$, 因此，$rank(B)=2$。
矩阵C，没有一行（或一列）是其他行（或列）的线性组合。因此，$rank(C)=3$。

不难发现，​秩中隐含了对“信息量”的提示​。

### 奇异值分解

奇异值分解(SVD)是矩阵分解的一种形式，具有明确的数学定义。以下是对奇异值分解的数学定义：

奇异值分解是指将一个秩为r的实矩阵A（大小为m×n）分解为三个实矩阵乘积的形式：

$$
A = U \Sigma V^T
$$

其中：

* $U$是一个$m×m$的正交矩阵，其列向量称为左奇异向量。
* $V$是一个$n×n$的正交矩阵，其列向量称为右奇异向量。
* $\Sigma$是一个$m×n$的矩形对角矩阵（或称为对角块矩阵），称为奇异值矩阵。其对角线上的元素（或对角块）称为奇异值，且这些奇异值是非负的，并按照降序排列（尽管在数学定义中不强制要求降序排列，但在实际应用中通常会这样做以便于分析和处理）。

举个具体的例子，大家体会一下SVD。下面这个例子是针对矩阵$A$的奇异值分解$(A = U\Sigma V^T)$：

$$
\begin{aligned}
A & = U\Sigma V^T \\
\begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix} 
& = 
\begin{bmatrix}-0.408&-0.707&0.569\\-0.577&0.000&-0.816\\-0.745&0.707&0.195\end{bmatrix} 
\begin{bmatrix}14.26&0&0\\0&1.5&0\\0&0&0.56\end{bmatrix} 
\begin{bmatrix}-0.237&-0.520&-0.803\\-0.689&-0.226&0.697\\0.685&-0.821&0.078\end{bmatrix} \\
& = 
\begin{bmatrix}-0.2148&-0.8872&0.4082\\-0.5206&-0.2496&-0.8165\\-0.8263&0.3880&0.4082\end{bmatrix}
\begin{bmatrix}16.8481&0&0\\0&1.0684&0\\0&0&0\end{bmatrix}
\begin{bmatrix}-0.4797&-0.5724&-0.6651\\-0.7767&-0.0757&0.6348\\-0.4082&0.8165&-0.4082\end{bmatrix}
\end{aligned}
$$

Tip: 矩阵的奇异值分解不唯一。

### 矩阵降维

介绍完秩和SVD之后，我们来继续讨论一下我们关心的问题，如何把一个庞大的矩阵，变成一个较小的矩阵，来达到简化运算的目的。这个思路就类似于，上面那个初级数学中找个接近的近似数来代替精确值的办法。
我们把这个问题，用具体的例子来表述一下。

$$
A = \left[\begin{matrix}1 & 0 & \cdots & 2 \\ 0 & 1 & \cdots & 3 \\ \vdots & \vdots & \ddots & \vdots \\ 4 & 5 & \cdots & 0  \end{matrix}\right]_{90,000 \times 1,000} 
\longrightarrow \qquad

A' = \left[\begin{matrix}1 & 0 & \cdots & 2 \\ 0 & 1 & \cdots & 3 \\ \vdots & \vdots & \ddots & \vdots \\ 4 & 5 & \cdots & 0  \end{matrix}\right]_{900 \times 10}
$$

我们需要用一个算法，找到A'，然后用A‘来代替A，从而极大的降低计算成本。这个过程，用洋气且专业的名字叫“降维”，即把矩阵的行和列，压缩一下（2个数量级及以上）。
现在继续上面这个3阶矩阵

$$
A = \begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix} = 
\begin{bmatrix}-0.2148&-0.8872&0.4082\\-0.5206&-0.2496&-0.8165\\-0.8263&0.3880&0.4082\end{bmatrix}
\begin{bmatrix}16.8481&0&0\\0&1.0684&0\\0&0&0\end{bmatrix}
\begin{bmatrix}-0.4797&-0.5724&-0.6651\\-0.7767&-0.0757&0.6348\\-0.4082&0.8165&-0.4082\end{bmatrix}
$$

* **降维操作**：由于$\Sigma$中第三个奇异值为$0$，可以将其对应的列和行向量舍去，实现矩阵降维。保留前两个奇异值及其对应的向量，得到降维后的矩阵表示：

$$
\begin{aligned}
A_{approx} & =  U_{approx}\Sigma_{approx}V_{approx}^T \\ where \quad          
& U_{approx}=\begin{bmatrix}-0.2148&-0.8872\\-0.5206&-0.2496\\-0.8263&0.3880\end{bmatrix} \\
& \Sigma_{approx}=\begin{bmatrix}16.8481&0\\0&1.0684\end{bmatrix} \\
& V_{approx}^T=\begin{bmatrix}-0.4797&-0.5724\\-0.7767&-0.0757\end{bmatrix}
\end{aligned}
$$

* **降维效果**：
  计算$A_{approx}$, 可得$ A_{approx}=\begin{bmatrix}1.0485&2.0583&3.0681\\4.1334&5.1432&6.1530\\7.2183&8.2281&9.2379\end{bmatrix} $ ，与原矩阵$A=\begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix}$相比，在一定程度上保留了原矩阵的主要信息，但维度从$3\times3$降到了$3\times2$与$2\times2$与$2\times3$的组合，实现了降维的目的。

在实际应用中，通常会根据奇异值的大小来决定保留多少维度。如果奇异值衰减得很快，只保留少数几个较大的奇异值及其对应的向量就可以很好地近似原矩阵，从而达到降维并减少数据量的效果。推广到一般情况，我们常常用SVD来做矩阵的行压缩和列压缩。

* **行压缩**：通过考虑$U$和$\Sigma$的前$k$个奇异值和对应的左奇异向量，可以将数据的每一列（特征）投影到一个低维空间，实现行数据的降维。
* **列压缩**：聚焦于$\Sigma$和$V$，通过选择$\Sigma$的前$k$个非零奇异值和$V$的前$k$列，可以将数据的每一行（样本）投影到低维空间，实现列数据的降维。


### 再谈全参数微调

我们再用上面的方法，把全参数微调表示出来。如下图所示，全参数微调其实就是把权重矩阵$W$训练成$W'$。

$$
W = \left[\begin{matrix}w_{11} & w_{12} & \cdots & w_{1d} \\ w_{21} & w_{22} & \cdots & w_{2d} \\ \vdots & \vdots & \ddots & \vdots \\ w_{d1} & w_{d2} & \cdots & w_{dd}  \end{matrix}\right]_{d \times d} 

\qquad \longrightarrow \qquad

W' = \left[\begin{matrix}w'_{11} & w'_{12} & \cdots & w'_{1d} \\ w'_{21} & w'_{22} & \cdots & w'_{2d} \\ \vdots & \vdots & \ddots & \vdots \\ w'_{d1} & w'_{d2} & \cdots & w'_{dd}  \end{matrix}\right]_{d \times d}
$$

现在，我们再做一个简单的数学转化：

$$
\Delta W = W'-W 

= \left[\begin{matrix}w'_{11} & w'_{12} & \cdots & w'_{1d} \\ w'_{21} & w'_{22} & \cdots & w'_{2d} \\ \vdots & \vdots & \ddots & \vdots \\ w'_{d1} & w'_{d2} & \cdots & w'_{dd}  \end{matrix}\right]_{d \times d} 
- \quad
 \left[\begin{matrix}w_{11} & w_{12} & \cdots & w_{1d} \\ w_{21} & w_{22} & \cdots & w_{2d} \\ \vdots & \vdots & \ddots & \vdots \\ w_{d1} & w_{d2} & \cdots & w_{dd}  \end{matrix}\right]_{d \times d} 
= \quad 
\left[\begin{matrix}\Delta w_{11} & \Delta w_{12} & \cdots & \Delta w_{1d} \\ \Delta w_{21} & \Delta w_{22} & \cdots & \Delta w_{2d} \\ \vdots & \vdots & \ddots & \vdots \\ \Delta w_{d1} & \Delta w_{d2} & \cdots & \Delta w_{dd}  \end{matrix}\right]_{d \times d}
$$

做了这个数学变化后，微调训练的问题，就转化为找到一个矩阵$\Delta W$, 然后可以简单的做一个加法来算出最终我们要算出来的$W'$，即$W' = W + \Delta W$。

#### 聚焦矩阵$\Delta W$

如果我们想训练出$\Delta W$，因为是$d \times d $维度的，代价不菲。参考前文提及到的数学基础知识，自然而然，我们就想到针对矩阵$\Delta W$降维。既然SVD分解这么有效，那我们直接对$\Delta W$做SVD，找到对应的低秩矩阵，不就大功告成了吗？
现实很骨感，我们bing

​**想法虽然好，但困难是明显的：能直接做SVD的前提是是确定的**​，而现实中作为全参数微调中的权重增量，如果你不全参数微调一遍，又怎么能知道长什么样呢？而如果你做了全量微调，那还要低秩适配做什么呢？

欸，你可能又想：那我能不能对预训练权重做SVD呢，因为是确定的呀。

​**想法虽然好，但逻辑是不合理的：我们说过，微调的目的是给模型注入和下游任务相关的领域新知识**​。也就是说，和​**的表达含义是不同的，前者是新知识，后者是旧知识**​，我们的目的是要去新知识中拆解信息量丰富的维度。

好，​**那既然通过数学方法直接做SVD行不通，那就让模型自己去学怎么做SVD吧**​！因此LoRA最终的低秩适配策略是：我把秩当成一个超参，再让模型自己去学低秩矩阵，这不就简单又省事吗！

行，到这里我们已经具象化地了解了LoRA低秩适配的原理了，也知道和所表达含义的差异了，**现在，我们可以来看前文遗留的问题：超参是什么意思？**



## 参考文献

1. [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647)

