# 详谈Skip-gram

前文介绍WordEmbedding是留下了一个坑，即CBOW和Skip-gram算法。
这两个算法正好是个逆过程，实际效果来说，Skip-gram要好一些。因此，本文里将详细探讨Skip-gram算法。

## 写在前面

学生时代，每次读论文时，我都会跳过Introduction/Background部分，直接看Solution部分。随着年纪的增长，开始注重阅读这部分内容，即提出问题部分。而在学习Solution部分时，更加注重论文作者为什么会想到这个办法来解决问题。实话实说，用这种方式读论文，效率非常低，就笔者自己来说，看论文的速度以及比不上学生时代十分之一了。但是，依旧乐此不疲……

书接上文，闲话少叙。

## 提出问题：如何用数学表示语义信息

我们上文提到一个关于词向量的结论，基于分布式假说思想，就把语义表达的问题就简化了，即学习词之间的搭配关系。

> 词向量模型就是学习词(Tokens)之间的搭配关系

## 解决问题：

### 一个具体的例子

我们用一个例子来说明。

已知：现在有四个词（Token），分别是$a,b,c,d$。现在我们要表达下面这个搭配关系：
求：(I) $\{a,b\}$，$\{b,c\}$，以及$\{c,d\}$有搭配关系

解：(I) 我们用$Y$表示:有搭配关系；用$N$表示:没有搭配关系。
我们就可以用下面这个二维的表格来求解。

|  | a | b | c | d |
| -- | -- | -- | -- | -- |
| a | N/A | Y | N | N |
| b | Y | N/A | N | N |
| c | N | Y | N/A | Y |
| d | N | N | Y | N/A |

现实的世界往往比这个更加复杂，比如我们需要表达这样的语义：

* a和b最像，a和c次之，a和d最不像。

如何用数学来表达这样的概念呢？这里，我们就要引入概率这个数学工具了。现在我们将上述的语义转化为概率。

$$
P_{(a,b)} = 0.6 \\
P_{(a,c)} = 0.3 \\
P_{(a,d)} = 0.1 \\
其中， \sum_{i}^{\{b,c,d\}} P_{(a,i)} = 1
$$

我们也用概率方式，把(b,c)和(c,d)关系也表示出来。这样上面的二维表格，就可以表示为：

|  | a | b | c | d |
| -- | -- | -- | -- | -- |
| a | N/A | 0.6 | 0.3 | 0.1 |
| b | 0.6 | N/A | 0.1 | 0.3 |
| c | 0.3 | 0.1 | N/A | 0.6 |
| d | 0.1 | 0.3 | 0.6 | N/A |

接下来我们把上述例子推广的更加一般的情况，专业术语叫“泛化”。

### 更加一般的例子

先回顾一下BoW的工作流程，如下：

![WordPresentation_Workflow.svg](../images/WordPresentation_Workflow.svg)

第一步：分词，即将原始的文本，改成计算机统一能处理的最小单位Token的集合。
第二步：统计各Token在文本中出现的频率，得到基于频率的词汇表。
第三步：参考第二步得到基于频率的词汇表，将第一步得到Tokens表示后的文本，用向量化表示出来。

其中，第二部提到了一个非常重要的概念——词汇表(Vocabulary Table)。我们用的Tokens是可以用有限个词，表述出来的。如果结合到上述二维表格，换句话说，是可以用有限的行和列表示出来的。
这样，假设我们的词汇表，一共有10，000个词。上面的二维表格就可以表示为，如下图：

|  | $W_1$ | $W_2$ | ... | $W_{10,000}$ |
| -- | -- | -- | -- | -- |
| $W_1$ | N/A | $P_{(w_1,w_2)}$ | ... | $P_{(w_1,w_{10,000})}$ |
| $W_2$ | $P_{(w_1,w_2)}$ | N/A | ... | $P_{(w_2,w_{10,000})}$ |
| ... | ... | ... | ... | ... |
| $W_{10,000}$ | $P_{(w_1,w_{10,000})}$ | $P_{(w_2,w_{10,000})}$ | ... | N/A |

至此，我们就解决用数学表示语义的问题。

