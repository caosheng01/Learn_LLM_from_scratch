# 详谈Skip-gram

前文介绍WordEmbedding是留下了一个坑，即CBOW和Skip-gram算法。
这两个算法正好是个逆过程，实际效果来说，Skip-gram要好一些。因此，本文里将详细探讨Skip-gram算法。

## 写在前面

学生时代，每次读论文时，我都会跳过Introduction/Background部分，直接看Solution部分。随着年纪的增长，开始注重阅读这部分内容，即提出问题部分。而在学习Solution部分时，更加注重论文作者为什么会想到这个办法来解决问题。实话实说，用这种方式读论文，效率非常低，就笔者自己来说，看论文的速度以及比不上学生时代十分之一了。但是，依旧乐此不疲……

书接上文，闲话少叙。

## 提出问题：如何用数学表示语义信息

我们上文提到一个关于词向量的结论，基于分布式假说思想，就把语义表达的问题就简化了，即学习词之间的搭配关系。

> 词向量模型就是学习词(Tokens)之间的搭配关系

## 解决问题

### 一个具体的例子

我们用一个例子来说明。

已知：现在有四个词（Token），分别是$a,b,c,d$。现在我们要表达下面这个搭配关系：
求：(I) $\{a,b\}$，$\{b,c\}$，以及$\{c,d\}$有搭配关系

解：(I) 我们用$Y$表示:有搭配关系；用$N$表示:没有搭配关系。
我们就可以用下面这个二维的表格来求解。

|   | a   | b   | c   | d   |
| - | --- | --- | --- | --- |
| a | N/A | Y   | N   | N   |
| b | Y   | N/A | N   | N   |
| c | N   | Y   | N/A | Y   |
| d | N   | N   | Y   | N/A |

现实的世界往往比这个更加复杂，比如我们需要表达这样的语义：

* a和b最像，a和c次之，a和d最不像。

如何用数学来表达这样的概念呢？这里，我们就要引入概率这个数学工具了。现在我们将上述的语义转化为概率。

$$
P_{(a,b)} = 0.6 \\
P_{(a,c)} = 0.3 \\
P_{(a,d)} = 0.1 \\
其中， \sum_{i}^{\{b,c,d\}} P_{(a,i)} = 1
$$

我们也用概率方式，把(b,c)和(c,d)关系也表示出来。这样上面的二维表格，就可以表示为：

|   | a   | b   | c   | d   |
| - | --- | --- | --- | --- |
| a | N/A | 0.6 | 0.3 | 0.1 |
| b | 0.6 | N/A | 0.1 | 0.3 |
| c | 0.3 | 0.1 | N/A | 0.6 |
| d | 0.1 | 0.3 | 0.6 | N/A |

接下来我们把上述例子推广的更加一般的情况，专业术语叫“泛化”。

### 更加一般的例子

先回顾一下BoW的工作流程，如下：

![WordPresentation_Workflow.svg](../images/WordPresentation_Workflow.svg)

第一步：分词，即将原始的文本，改成计算机统一能处理的最小单位Token的集合。
第二步：统计各Token在文本中出现的频率，得到基于频率的词汇表。
第三步：参考第二步得到基于频率的词汇表，将第一步得到Tokens表示后的文本，用向量化表示出来。

其中，第二部提到了一个非常重要的概念——词汇表(Vocabulary Table)。我们用的Tokens是可以用有限个词，表述出来的。如果结合到上述二维表格，换句话说，是可以用有限的行和列表示出来的。假设，词汇表里面有n个词，上面的二维表格就可以表示为，如下图：

|              | $W_1$                  | $W_2$                  | ... | $W_n$           |
| ------------ | ---------------------- | ---------------------- | --- | ---------------------- |
| $W_1$        | N/A                    | $P_{(w_1,w_2)}$        | ... | $P_{(w_1,w_n)}$ |
| $W_2$        | $P_{(w_1,w_2)}$        | N/A                    | ... | $P_{(w_2,w_n)}$ |
| ...          | ...                    | ...                    | ... | ...                    |
| $W_n$ | $P_{(w_1,w_n)}$ | $P_{(w_2,w_n)}$ | ... | N/A                    |

Tips：以开源的词汇表[vocab.txt](https://huggingface.co/google-bert/bert-base-cased/blob/main/vocab.txt) 为例，一共有28,998个词，此时n就等于28，998。

至此，我们就解决用数学表示语义的问题。

### 用计算机技术来解决问题

当我们解决了语义表示的问题后，该如何算出概率值呢？在BoW模型中，是基于频率来计算概率值。全局上，必须维护这一张表格。如果要表示$W_i$和$W_j$的关系，只需要查表，取到$P_{(w_i,w_j)}$的值即可。使用上，非常方便。但是，有个致命的缺点，就是无法表示“一词多义”。换句话说，$W_i$和$W_j$的关系一定等于$P_{(w_i,w_j)}$的值。
“一词多义”的问题，我们后面再讨论，现在先解决如何在计算机里面表示二维表格的问题。
我们先取$W_1$行或者列，用数学来表示，就是下面这个矩阵。

$$
W_1 =\left[\begin{matrix}
      0 \\
      P_{(w_1,w_2)} \\
      P_{(w_1,w_3)} \\
      ... \\
      P_{(w_1,w_n)}
      \end{matrix}\right] \\
      
其中， \sum_{i=1}^{n} P_{(w_1,w_i)} = 1
$$

上面这个表示法，隐含的意思就是$W_1$和词汇表里其他词所蕴含的语义。而这个结构，对应计算机里的数据结构就是一维数组，即向量。这样，我们就把一个人类的语言（Word），通过分词（Tokenizer）转换为词（Token），然后转换为了向量（Vector），这个过程被形象的成为“Word to Vector”。

我们还是以上面这个$W_1$为例，来详细描述“Word to Vector”的过程，如下图所示：
![Skip_gram_w1_1.svg](../images/Skip_gram_w1_1.svg)

因为最终输出是一个概率值，且总和为1，我们很容易想到用Softmax函数。同时，我们把$W_1$用独热码做一个编码，上面这个过程就变成了下图：
![Skip_gram_w1_2.svg](../images/Skip_gram_w1_2.svg)

现在实现Word2Vec的整体流程，我们还剩下最后一步，找到一个数学模型（即，用函数来描述）。一般来说，数学模型就是一个或者一组数学公式。因为现实世界过于复杂，找到一组精确的数学公式来描述，非常难。随着人工智能邻域的神经网络技术发展，我们一般直接训练一个神经网络来对应。所以上图，笔者就直接用神经网络（NN）来代替函数（Function）了。接下来，我们就动手设计一个NN模型。

#### 动手设计神经网络模型

我们来看看，这个数学模型要做什么事情。这个NN需要把输入$W_1$和词汇表中的其他词，即$W_2,W_3,...,W_n$，的搭配关系表述出来。
我们来看一下这个模型需要哪些功能：

**模型设计目标**：

- 主要目标是学习词向量，即把词映射到向量空间中的点，使得具有相似含义或上下文的词在向量空间中的位置相近。
- 这个目标并不需要复杂的网络结构或深层的隐藏层来达成，因为主要的任务是进行词汇间的相似性比较，而不是进行复杂的模式识别或特征提取。
- 尽量减少模型的参数量，提高训练效率。
- 因为输入为独热码，存在大量的稀疏矩阵，需要这个模型能降维，且不需要非线性变换，即不需要激活函数。

综上，该模型只需要1层隐藏层就足够了，这既是因为模型的设计目标相对简单，也是因为单层隐藏层已经能够在保证性能的同时提高训练效率。

