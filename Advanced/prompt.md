# 聊聊提示词工程

当ChatGPT破圈后，越来越多的人涌入了AI赛道，这使得AI培训课程火了起来。在最初的一年里，绝大多少的AI课程内容都是围绕着提示词工程（Prompt Engineering）。这现象像极上世纪90年代初，当PC开始普及时，人们开始学习如何使用操作系统（Windows），如何输入办公（Office）等。
作为一个技术出身的人，笔者也从来没有想过自己会以提示词工程为内容写一篇技术专题。最初觉得提示词好需要花时间学？后来实际工作需要，发现好的提示词对生产力的提升是显而易见的，故沉下心来好好看看相关的技术文章。结果那是被网上的技术文章看的眼花缭乱，太多的框架和建议了，我记性又差，现在回想起来真是一段非常不好的学习经历。
最后决定自己写一篇吧，类似于读书心得。那么既然决定要写，就要写一些适合技术人员看的Prompt Engineering。因此，本文将从程序员的视角来聊Prompt。

## 什么是提示词工程

抛开枯燥的定义，我们先来看看人类是如何寻求答案/解决问题这个过程。这个过程可以抽象成一个简单的图，如下图所示。

![Prompt_human.svg](../images/Prompt_human.svg)

这是一个典型的输入，处理，输出的过程。那么我们如何将这个过程，用数学语言来表达呢？在数学上，含有输入/处理/输出这个过程的，现成的工具就是函数，如下图所示。

![Prompt_math.svg](../images/Prompt_math.svg)

既然能用数学表达，那么我们就能用代码将这个问题实现，过程如下图所示。

![Prompt_code.svg](../images/Prompt_code.svg)

在这个过程中，程序员的工作，就是把数学表达式/逻辑，用代码表示出来，从而求得答案。但是这个过程中，遇到了一些问题，比如图像识别等抽象的工作很难用一组数学表达式来表示。幸运的是，我们找到了办法，用神经网络NN来解决这类问题，我们也进入以LLM为代表的AI时代，这个问题就可以用下图来表示。

![Prompt_nn.svg](../images/Prompt_nn.svg)

这里出现了提示词(Prompt)，如图所示，Prompt就是LLM的输入，以自然语言表示。而提示词工程的最终目标就是如何提高提示词的实际效果。

在基础篇《》中，笔者提到一个解决问题的思路，在遇到工程问题时，参考别的领域中类似问题的解决思路，然后复制到本领域中。学习上，这个跨学科的思路，也可以借鉴。对程序员来说，最熟悉的就是写代码。那么我们就以如何写一个函数为例，来学习提示词工程。
如下图所示，在成熟项目里，写一个合格的函数，有下面5个典型的步骤。

![Prompt_function.svg](../images/Prompt_function.svg)

1. **准备阶段**， 在这个阶段，程序员一般会配置软件的运行环境，debug环境等。
2. **设计阶段**， 这个阶段，定义函数做具体完成的任务，包括接口定义，函数名称，返回值等。
3. **实现阶段**， 这个阶段，就是按照设计阶段的任务，做具体的实现，代码需要符合团队的代码规范(Code Style Spec.)，软件漏洞扫描等。
4. **重构阶段**， 这个阶段不是必须的，在成熟的软件团队中，会对每一个提交的代码(Pull Request)做审核(Review)。
5. **测试阶段**， 这个阶段就是单元测试，大型项目中，Unit test必须和代码一起提交，以保证最基本的代码质量。

提示词工程的学习，笔者也把提示词工程分为这5个阶段，现在我们来逐一介绍。

## 提示词工程：准备阶段

大语言模型（LLM）都提供了多种配置选项，用于控制其输出内容。有效的提示词工程需要针对具体任务优化这些配置。输出配置主要分为两类，一类是输出长度(Output Length)，一类是采样控制(Sampling Control)

### 输出长度

顾名思义，这一配置就是设置是LLM生成的token数量。生成更多tokens需要LLM进行更多计算，这会导致能耗增加、响应速度可能变慢，且成本更高。减少LLM的输出长度并不会使模型在输出风格或文本表达上更简洁，它只会让模型在达到长度限制后停止预测更多tokens。输出长度限制对于某些LLM提示技术尤为重要，例如ReAct技术——在这类场景中，LLM可能会在生成所需响应后继续输出无用的tokens。

### 采样控制

大语言模型并非直接预测单个token，而是预测下一个可能出现的token的概率。具体的讲，LLM先把词汇表中的每个 token都会被分配一个概率。随后，LLM会基于这些token概率进行采样，以确定下一个输出的token。
其中温度(Temperature)、Top-K和Top-P是最常用的配置参数，它们决定了LLM如何基于概率来选择单个输出token。

#### 温度(Temperature)

温度用于控制token选择过程中的随机性。较低的温度适用于需要确定性响应的提示词场景，而较高的温度则能带来更多样化或意想不到的结果。温度为0时(即贪婪解码)，输出具有确定性：LLM总会选择概率最高的token。温度越高，输出越随机。下面是关于温度的数学描述，想深入这一问题的读者，对着公式再体会一下温度的含义：用于控制token选择过程中的随机性。

**温度**是通过调整softmax函数来控制token概率分布的平滑度的参数。其数学公式如下：

对于模型预测的原始logits $ z_1, z_2, …, z_n$（其中n是词汇表大小），应用温度T后的概率P分布为：

$$
P(i) = \frac{\exp(\frac{z_i}{T})}{\sum_{j=1}^{n} \exp(\frac{z_j}{T})}
$$

其中：

* $z_i$是第 $i$个token的原始预测得分(logit)。
* $T$是温度参数，必须大于0, 在LLM配置中，建议取值范围在0~1之间。
* $exp$ 表示自然指数函数。

温度对概率分布的影响：

* $T→0$​（趋近于 0）：
  * 概率分布变得非常 “尖锐”，模型几乎总是选择概率最高的token（即贪婪解码）。
  * 极端情况下（$T=0$），直接选择最大logit对应的token(实际实现中通常用$T=0.01$近似）。
* ​$T=1$​：
  * 等同于标准softmax函数，不改变原始预测的相对概率。
* ​$T→\infty$​（趋近于无穷大）：
* 概率分布变得均匀，所有token被选中的概率几乎相等，输出完全随机。

#### Top-K与Top-P

Top-K和Top-P是大语言模型中用于限制下一个预测token 范围的两种采样设置，它们会从预测概率较高的token中筛选候选。与温度类似，这些采样设置也会影响生成文本的随机性和多样性。

* ​**Top-K**​：从模型的预测分布中选择概率最高的前K个token。Top-K值越高，模型输出越具创造性和多样性；Top-K值越低，输出越稳定、更贴近事实。
* ​**Top-P**​：选择累计概率不超过某个阈值的token集合。P的取值范围为0（贪婪解码）到1（包含词汇表中所有 token）。

举个例子，如下表所示，一个完整的词表中，共有10个token。

| **Token**  | $z_1$ | $z_2$ | $z_3$ | $z_4$ | $z_5$ | $z_6$ | $z_7$ | $z_8$ | $z_9$ | $z_{10}$ |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| **logits**  | 0.15 | 0.10 | 0.05 | 0.20 | 0.21 | 0.11 | 0.09 | 0.04 | 0.07 | 0.08 |

若设置Top-K=3，则选出的Token集合是$\{z_5, z_4, z_1\}$
若设置Top-P=0.7，则选出的Token集合是$\{z_5, z_4, z_1，z_6\}$。具体计算逻辑是先按照Logits从高到低排序，然后按照顺序计算累计的logits，$0.21 + 0.20 + 0.15 + 0.11 = 0.67 < 0.7 $

Tips: 具体项目中，到底是选择Top-K还是Top-P，没有定论。最佳方式是试出来。即对两种方法(或两者结合)进行实验，观察哪种方式能得到预期结果。

#### 综合配置

Temperature、Top-K、Top-P以及生成token数量的选择取决于具体应用场景和预期结果，且这些设置之间会相互影响。如果同时配置这三个参数，那么会对模型输出有什么具体影响呢？想回答这个问题，我们就需要了解LLM的Transformer实现了。想进一步深入的读者，可以直接看HuggingFace的Transformer源码。这里，笔者简单的说一下结论。Top-K和Top-P在做Softmax之前，所以比起Temperature参数，LLM会先优先计算Top-K和Top-P。而Top-K和Top-P之间是AND的关系。即结果集需要同时满足Top-K和Top-P的筛选条件。

1. 若模型同时支持温度、Top-K和Top-P，则下一个预测 token 的候选范围需同时满足Top-K和Top-P的筛选条件，之后再通过温度从这些候选token中采样。
2. 若仅支持 Top-K 或 Top-P，则仅基于该单一设置筛选候选 token。
3. 若模型不支持温度调节，则会从满足 Top-K 和 / 或 Top-P 条件的 token 中随机选择下一个预测 token。

#### 极端值分析和参考配置

当某一采样配置参数处于极端值时，该参数可能会抵消其他配置的作用或使其他配置变得无关紧要：

* 若将温度设为0，Top-K和Top-P会变得无关 —— 概率最高的token会直接成为下一个预测 token。若将温度设为极高值（超过 1，通常达到 10 以上），温度的作用会失效，此时会从通过Top-K和Top-P筛选的token中随机采样下一个预测token。
* 若将Top-K设为1，温度和Top-P会变得无关 —— 只有一个token能通过Top-K筛选，该token即为下一个预测token。若将Top-K设为极高值（如等于模型词汇表的大小），则所有具有非零概率的token都能通过Top-K筛选，此时该设置相当于无效。
* 若将Top-P设为0（或极小值），大多数LLM采样机制会仅考虑概率最高的token满足Top-P条件，导致温度和Top-K变得无关。若将Top-P设为1，则所有具有非零概率的token都能通过Top-P筛选，此时该设置相当于无效。

作为通用的初始配置，建议温度设为0.2、Top-P设为0.95、Top-K设为30，通常能得到相对连贯的结果，既有一定创造性又不会过于发散。如果你需要特别有创造性的结果，可以尝试从温度设为0.9、Top-P设为0.99、Top-K设为40，开始调试。而如果你希望结果的创造性更低（更稳定），可以从温度设为0.1、Top-P设为0.9、Top-K设为20开始尝试。最后，如果你的任务存在唯一正确答案（例如解答数学题），建议从温度0开始配置。

