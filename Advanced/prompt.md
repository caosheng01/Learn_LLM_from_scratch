# 聊聊提示词工程

当ChatGPT破圈后，越来越多的人涌入了AI赛道，这使得AI培训课程火了起来。在最初的一年里，绝大多少的AI课程内容都是围绕着提示词工程（Prompt Engineering）。这现象像极上世纪90年代初，当PC开始普及时，人们开始学习如何使用操作系统（Windows），如何输入办公（Office）等。
作为一个技术出身的人，笔者也从来没有想过自己会以提示词工程为内容写一篇技术专题。最初觉得提示词好需要花时间学？后来实际工作需要，发现好的提示词对生产力的提升是显而易见的，故沉下心来好好看看相关的技术文章。结果那是被网上的技术文章看的眼花缭乱，太多的框架和建议了，我记性又差，现在回想起来真是一段非常不好的学习经历。
最后决定自己写一篇吧，类似于读书心得。那么既然决定要写，就要写一些适合技术人员看的Prompt Engineering。因此，本文将从程序员的视角来聊Prompt。

## 什么是提示词工程

抛开枯燥的定义，我们先来看看人类是如何寻求答案/解决问题这个过程。这个过程可以抽象成一个简单的图，如下图所示。

![Prompt_human.svg](../images/Prompt_human.svg)

这是一个典型的输入，处理，输出的过程。那么我们如何将这个过程，用数学语言来表达呢？在数学上，含有输入/处理/输出这个过程的，现成的工具就是函数，如下图所示。

![Prompt_math.svg](../images/Prompt_math.svg)

既然能用数学表达，那么我们就能用代码将这个问题实现，过程如下图所示。

![Prompt_code.svg](../images/Prompt_code.svg)

在这个过程中，程序员的工作，就是把数学表达式/逻辑，用代码表示出来，从而求得答案。但是这个过程中，遇到了一些问题，比如图像识别等抽象的工作很难用一组数学表达式来表示。幸运的是，我们找到了办法，用神经网络NN来解决这类问题，我们也进入以LLM为代表的AI时代，这个问题就可以用下图来表示。

![Prompt_nn.svg](../images/Prompt_nn.svg)

这里出现了提示词(Prompt)，如图所示，Prompt就是LLM的输入，以自然语言表示。而提示词工程的最终目标就是如何提高提示词的实际效果。

在基础篇《》中，笔者提到一个解决问题的思路，在遇到工程问题时，参考别的领域中类似问题的解决思路，然后复制到本领域中。学习上，这个跨学科的思路，也可以借鉴。对程序员来说，最熟悉的就是写代码。那么我们就以如何写一个函数为例，来学习提示词工程。
如下图所示，在成熟项目里，写一个合格的函数，有下面5个典型的步骤。

![Prompt_function.svg](../images/Prompt_function.svg)

1. **准备阶段**， 在这个阶段，程序员一般会配置软件的运行环境，debug环境等。
2. **设计阶段**， 这个阶段，定义函数做具体完成的任务，包括接口定义，函数名称，返回值等。
3. **实现阶段**， 这个阶段，就是按照设计阶段的任务，做具体的实现，代码需要符合团队的代码规范(Code Style Spec.)，软件漏洞扫描等。
4. **重构阶段**， 这个阶段不是必须的，在成熟的软件团队中，会对每一个提交的代码(Pull Request)做审核(Review)。
5. **测试阶段**， 这个阶段就是单元测试，大型项目中，Unit test必须和代码一起提交，以保证最基本的代码质量。

提示词工程的学习，笔者也把提示词工程分为这5个阶段，接下来我们将逐一介绍。不过，在介绍提示词工程前，先来解决一个问题：提示词工程为什么有用？有什么理论依据吗？

## 提示词工程：原理

用一句话来阐述提示词工程的原理，就是让提示词尽量和人类对LLM做训练时的训练数据接近。

在基础篇《大语言模型：通往通用人工智能之路》里，笔者以OpenAI的GPT线索详细介绍LLM的发展历程。简单的复述一下发展经历及对于提示词工程的意义。

1. GPT-1： 成功的把Transfermation这个特征提取器引入模型，用上了两阶段训练的方法，这个版本的GPT本质上还是分类模型。从提示词的角度讲，GPT-1对提示词工程没有特殊贡献。
2. GPT-2：用上了自监督学习。其设计思路就是结构化SFT的训练数据，核心结构就是，**指令**(目标任务) + **示例**(目标示例)。从提示词的角度讲，引入了Zero-Shot prompt（只有指令）和Few-shot prompt（指令+示例）。对我们的提示，就是在使用LLM过程中，提示词要尽量多的提供示例。
3. GPT-3： 用了更大的参数模型和更多的数据，产生了“涌现”的现象。观察到了思维链(CoT)的能力。我们也常常把CoT的技巧用在提示词工程上。
4. GPT-3.5： 这个就是ChatGPT，主要就是把GPT-3和人类对齐。在提示语上，我们也常常给LLM设立各种不同的“人设”。

接下来，我们开始学习提示词工程。

## 提示词工程：准备阶段

大语言模型（LLM）都提供了多种配置选项，用于控制其输出内容。有效的提示词工程需要针对具体任务优化这些配置。输出配置主要分为两类，一类是输出长度(Output Length)，一类是采样控制(Sampling Control)

### 输出长度

顾名思义，这一配置就是设置是LLM生成的token数量。生成更多tokens需要LLM进行更多计算，这会导致能耗增加、响应速度可能变慢，且成本更高。减少LLM的输出长度并不会使模型在输出风格或文本表达上更简洁，它只会让模型在达到长度限制后停止预测更多tokens。输出长度限制对于某些LLM提示技术尤为重要，例如ReAct技术——在这类场景中，LLM可能会在生成所需响应后继续输出无用的tokens。

### 采样控制

大语言模型并非直接预测单个token，而是预测下一个可能出现的token的概率。具体的讲，LLM先把词汇表中的每个 token都会被分配一个概率。随后，LLM会基于这些token概率进行采样，以确定下一个输出的token。
其中温度(Temperature)、Top-K和Top-P是最常用的配置参数，它们决定了LLM如何基于概率来选择单个输出token。

#### 温度(Temperature)

温度用于控制token选择过程中的随机性。较低的温度适用于需要确定性响应的提示词场景，而较高的温度则能带来更多样化或意想不到的结果。温度为0时(即贪婪解码)，输出具有确定性：LLM总会选择概率最高的token。温度越高，输出越随机。下面是关于温度的数学描述，想深入这一问题的读者，对着公式再体会一下温度的含义：用于控制token选择过程中的随机性。

**温度**是通过调整softmax函数来控制token概率分布的平滑度的参数。其数学公式如下：

对于模型预测的原始logits $ z_1, z_2, …, z_n$（其中n是词汇表大小），应用温度T后的概率P分布为：

$$
P(i) = \frac{\exp(\frac{z_i}{T})}{\sum_{j=1}^{n} \exp(\frac{z_j}{T})}
$$

其中：

* $z_i$是第 $i$个token的原始预测得分(logit)。
* $T$是温度参数，必须大于0, 在LLM配置中，建议取值范围在0~1之间。
* $exp$ 表示自然指数函数。

温度对概率分布的影响：

* $T→0$​（趋近于 0）：
  * 概率分布变得非常 “尖锐”，模型几乎总是选择概率最高的token（即贪婪解码）。
  * 极端情况下（$T=0$），直接选择最大logit对应的token(实际实现中通常用$T=0.01$近似）。
* ​$T=1$​：
  * 等同于标准softmax函数，不改变原始预测的相对概率。
* ​$T→\infty$​（趋近于无穷大）：
* 概率分布变得均匀，所有token被选中的概率几乎相等，输出完全随机。

#### Top-K与Top-P

Top-K和Top-P是大语言模型中用于限制下一个预测token 范围的两种采样设置，它们会从预测概率较高的token中筛选候选。与温度类似，这些采样设置也会影响生成文本的随机性和多样性。

* ​**Top-K**​：从模型的预测分布中选择概率最高的前K个token。Top-K值越高，模型输出越具创造性和多样性；Top-K值越低，输出越稳定、更贴近事实。
* ​**Top-P**​：选择累计概率不超过某个阈值的token集合。P的取值范围为0（贪婪解码）到1（包含词汇表中所有 token）。

举个例子，如下表所示，一个完整的词表中，共有10个token。

| **Token**  | $z_1$ | $z_2$ | $z_3$ | $z_4$ | $z_5$ | $z_6$ | $z_7$ | $z_8$ | $z_9$ | $z_{10}$ |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| **logits**  | 0.15 | 0.10 | 0.05 | 0.20 | 0.21 | 0.11 | 0.09 | 0.04 | 0.07 | 0.08 |

若设置Top-K=3，则选出的Token集合是$\{z_5, z_4, z_1\}$
若设置Top-P=0.7，则选出的Token集合是$\{z_5, z_4, z_1，z_6\}$。具体计算逻辑是先按照Logits从高到低排序，然后按照顺序计算累计的logits，$0.21 + 0.20 + 0.15 + 0.11 = 0.67 < 0.7 $

Tips: 具体项目中，到底是选择Top-K还是Top-P，没有定论。最佳方式是试出来。即对两种方法(或两者结合)进行实验，观察哪种方式能得到预期结果。

#### 综合配置

Temperature、Top-K、Top-P以及生成token数量的选择取决于具体应用场景和预期结果，且这些设置之间会相互影响。如果同时配置这三个参数，那么会对模型输出有什么具体影响呢？想回答这个问题，我们就需要了解LLM的Transformer实现了。想进一步深入的读者，可以直接看HuggingFace的Transformer源码。这里，笔者简单的说一下结论。Top-K和Top-P在做Softmax之前，所以比起Temperature参数，LLM会先优先计算Top-K和Top-P。而Top-K和Top-P之间是AND的关系。即结果集需要同时满足Top-K和Top-P的筛选条件。

1. 若模型同时支持温度、Top-K和Top-P，则下一个预测 token 的候选范围需同时满足Top-K和Top-P的筛选条件，之后再通过温度从这些候选token中采样。
2. 若仅支持 Top-K 或 Top-P，则仅基于该单一设置筛选候选 token。
3. 若模型不支持温度调节，则会从满足 Top-K 和 / 或 Top-P 条件的 token 中随机选择下一个预测 token。

#### 参考配置

当某一采样配置参数处于极端值时，该参数可能会抵消其他配置的作用或使其他配置变得无关紧要：

* 若将温度设为0，Top-K和Top-P会变得无关 —— 概率最高的token会直接成为下一个预测 token。若将温度设为极高值（超过 1，通常达到 10 以上），温度的作用会失效，此时会从通过Top-K和Top-P筛选的token中随机采样下一个预测token。
* 若将Top-K设为1，温度和Top-P会变得无关 —— 只有一个token能通过Top-K筛选，该token即为下一个预测token。若将Top-K设为极高值（如等于模型词汇表的大小），则所有具有非零概率的token都能通过Top-K筛选，此时该设置相当于无效。
* 若将Top-P设为0（或极小值），大多数LLM采样机制会仅考虑概率最高的token满足Top-P条件，导致温度和Top-K变得无关。若将Top-P设为1，则所有具有非零概率的token都能通过Top-P筛选，此时该设置相当于无效。

作为通用的初始配置，建议温度设为0.2、Top-P设为0.95、Top-K设为30，通常能得到相对连贯的结果，既有一定创造性又不会过于发散。如果你需要特别有创造性的结果，可以尝试从温度设为0.9、Top-P设为0.99、Top-K设为40，开始调试。而如果你希望结果的创造性更低（更稳定），可以从温度设为0.1、Top-P设为0.9、Top-K设为20开始尝试。最后，如果你的任务存在唯一正确答案（例如解答数学题），建议从温度0开始配置。

## 提示词工程：设计阶段

现在让我们代入程序员视角，设计一个函数/API，让别人来调用，需要做什么。首先要识别调用者，即谁来使用这个函数。在实际工作中，我们常用Use Case图。接着，在你的设计文档，至少对以下3点要有清晰的说明：

1. ​**解释意图**​：说明函数 “为什么存在”，完成什么具体任务。
2. ​**指导使用**​：明确参数含义、返回值格式及潜在异常，一般还会有示例。
3. ​**提示边界**​：指出特殊输入（如空值、负数）的处理方式。

提示词工程，其实思路也差不多。先给LLM立个人设(System prompting)，然后告诉LLM具体要做什么事情(Contextual prompting)，最后规定输出的格式和风格(Role prompting)。

* **System Prompting**: 为LLM设定整体背景和目标。它定义了模型应执行任务的 “全局框架”，例如进行语言翻译、对评论进行分类等。
* **Contextual Prompting**: 提供与当前对话或任务相关的具体细节或背景信息。它帮助模型理解问题的细微差别，并据此调整响应内容。
* **Role Prompting**: 为LLM分配特定的角色或身份。这能帮助模型生成与所分配角色及其相关知识、行为一致的响应。

三者之间可能存在大量重叠。例如，一个为系统分配角色的提示，也包含了上下文信息。不过，每种提示的核心目的略有不同：

* ​**System Prompting**​：定义模型的基本能力和核心目标。
* ​**Contextual Prompting**​：提供即时的、特定于任务的信息以引导响应。它与当前任务或输入高度相关，具有动态性。
* ​**Role Prompting**​：塑造模型的输出风格和语气。它为响应增加了特定性和个性维度。

区分系统指令标识、情境(也叫上下文)提示和角色标识，可为设计具有明确意图的提示提供框架，支持灵活组合，并使分析每种提示类型对语言模型输出的影响变得更简单。
这里，比较容易混淆的时System Prompting和Role Prompting。我们直接以[OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat)为例, 来说说两者的区别。
在 ChatCompletion API 中，使用以下角色标识区分消息类型：

* ​**system**​：设置AI助手的整体行为和任务目标。
* ​**user**​：用户输入的内容。
* ​**assistant**​：模型生成的回复。

```json
[   
{"role": "system", "content": "你是一个乐于助人、富有创造力、准确且无害的人工智能助手。"},   
{"role": "user", "content": "你好！你能为我做些什么？"},   
{"role": "assistant", "content": "我能回答问题、生成文本、总结信息，以及做更多事情！今天我能为你提供什么帮助呢？"} 
]
```

这里system role更像一个全局设置，类似于代码中的全局变量，除非被覆盖，将对整个对话过程持续影响。而user role像一个局部变量，只船体用户的具体需求/交互指令。两者的核心区别如下表所示。

|        | role: "system"`             | role: "user"`               |
| ---------------------- | ---------------------------------- | ---------------------------------- |
| **本质**       | 模型的「行为规则设定」           | 用户的「具体需求输入」           |
| **作用对象**   | 约束模型的底层逻辑和风格         | 触发模型的具体响应内容           |
| **出现时机**   | 通常在对话开始时一次性设定       | 随对话进程动态输入（可多次发送） |
| **用户可见性** | 一般隐藏（模型内部参考）         | 完全可见（用户主动输入的内容）   |
| **示例内容**   | “你是数学老师，解题步骤要详细” | “帮我解这道方程：2x + 5 = 15”  |

## 提示词工程：实现阶段


## 提示词工程：调优阶段

## 提示词工程：测试阶段

## 提示词工程：最佳实践

## 参考文献

1. [Prompt Engineering](https://www.gptaiflow.com/assets/files/2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt%20Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf)

