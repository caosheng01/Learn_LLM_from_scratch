# 词表达考古史

## 提出问题

通用人工智能（AGI）的终极目标就是让计算机像人一样思考和说话。即让计算机能理解人类的语言，并且用人类的语言和人交流，和帮助人类处理各种事务。因此，我们提出了要解决的问题：

> 首当其冲要解决的问题，就是让计算机理解人类的语言

词表达（Word Presentation）就是指将自然语言中的词转换为机器可理解的形式或含义的过程。

## 解决问题

### 人类的语言

人类的语言有很多种，我们这里只讨论能表示成文字的那些语言。大致分为两个大类：**表音**和**表意**两类。
我们以英语（表音文字）和汉语（表意文字）为例,

* 英语的文本表示：字母 --> 单词 --> （词组） --> 句子
* 汉语的文本表示：字 --> （词语）--> 句子

文本的划分单元从小到大依次为单词、短语、句子、段落和篇章。其中，句子的表示法依赖于单词，而段落又依赖于句子，以此类推，可以说英文文本的表示是建立在单词的表示基础之上的。所以**单词的表示法**最为基础，也最为重要。相应的，中文中最重要的就是**汉字的表示法**。


### 计算机能处理的语言

众所周知，计算机只能处理0和1，即高低电平（两种状态）。所有的文本信息必须转化成数值型数据，才能被计算机处理。对于AI模型来说，非数值型的文本数据不能直接输入机器学习模型，要先经过编码转化成数值型数据才可用于模型训练或预测。而文本表示，就是研究如何将文本数据合理编码成数值型数据的技术。

### 计算机是如何表示文本

#### 编码表

理科研究，有个共性的办法。找到最小颗粒度的问题，然后研究透彻，然后在组合出复杂的问题。比如物理学，需要研究问题基本粒子的问题。比如生物学，我们需要研究基因。
计算机也不例外，以英文为例，我们先找到最小的单元：**字母**； 然后，把这个转化为0和1。这个过程，专业术语叫**编码**。
鼎鼎大名的[ASCII Table](https://www.ascii-code.com/)就是把字母，数字和特殊字符合在一起的编码表。比如：序号为97的字符a，对应的编码就是01100001。相应的，中文里也有类似的情况，比如GB2312编码表。随着计算机技术的普及，人们就有了统一各种语言的需求，于是乎，大家把各种主流语言的编码表做了一个统一，诞生了UTF-8这种字符表。

Tips:大家学习一下这种典型的计算机邻域解决问题的思路，当每个解决方案只能解决局部问题的时候，我们往往在这些解决方案上面加一层，用统一的办法来解决一类问题。软件领域，有个设计模式叫Adapter，就是这个思想。多层的网络协议栈，也是这种思路。

#### 词的表示方法

在自然语言中，文章是由段落构成，段落由句子组成，而句子由单词构成。然而，每个单词都有多种含义，因此在一片文章里，只看单词很难确定其所表达的含义。只有当单词放到句子中，结合上下文，才能确定单词所表达的含义，进而组合成句子，从而表达句子的语义。
基于这个原因，词表达不仅仅要把词本身用0，1表示出来，还需要结合上下文，精准的表示出其中的语义。
主流的词表达分为两类:

1. 词袋模型(Bag-of-words Model)
2. 词向量法(Word Embedding)

2024年，我们都采用就是词向量法。本文的重心也将放在介绍词向量法。现在，还有一个问题亟待解决。这个问题就是词(word)本身的定义。以中英文为例，如何让计算机统一处理英语单词（Word）和汉字？

#### 分词（Tokenizer）

如何把各种人类的语言变成统一格式，交给AI处理呢？我们先在计算机范畴内，找找看有没有类似的问题。
有了，计算机需要处理很多种编程语言，如C/Java/Go/Javascript等。和我们NLP里，遇到这个问题类似。我们一起来重温一下编译原理的处理过程。

![WordPresentation_Compile.svg](../images/WordPresentation_Compile.svg)

从上图可以看出，编译原理的第一步，词法分析就做了这个事情，把编程语言转化成Token，交给编译器处理。这个过程就叫分词（Tokenizer）。
同样的，在NLP种，我们也参考编译原理的处理方法。把各种人类的语言，转化为Token。交给AI来统一处理。

Tips: Token没有特别的中文翻译，后续文章提到词，指的就是统一格式的Token。

### 词袋模型(BoW:Bag-of-Words)

词袋模型是一种在自然语言处理（NLP）和信息检索（IR）中广泛使用的简化表达模型。举个例子，假如现在有1,000篇新闻文档，把这些文档拆成一个个的字，去重后得到3,000个字，然后把这3,000个字作为字典，进行文本表示的模型，叫做词袋模型。这种模型的特点是字典中的字没有特定的顺序，句子的总体结构也被舍弃了。在本章节里，我们将介绍三种最常用的，分别是one-hot（独热码），TF-IDF和n-gram。

这里，我们用一个简单的例子来讲解这三种方式。一个文本有两句话：

* 第一句：**小美女太可爱了，我爱小美女。**
* 第二句：**我要看小美女的演唱会。**

#### One-hot

针对上面提到的例子，One-hot是这样做的。

1. 先把这两句话拆成一个个字，一共有22个字。
   
   |小|美|女|太|可|爱|了|我|爱|小|美|女|我|要|看|小|美|女|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
   
   
2. 去重，整理得到不重复的字，一共有14个字。
   
   |小|美|女|太|可|爱|了|我|要|看|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
   
   
3. 统计第一句话中每个字在字典中出现的位置
   
   |小|美|女|太|可|爱|了|我|要|看|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
| 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |
   
   
4. 统计第二句话中每个字在字典中出现的位置，得到词汇频率表(Vocabulary Frequency Table)
   
   |小|美|女|太|可|爱|了|我|要|看|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
| 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |
| 1 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |
   
   
5. 向量化表示这两句话，得到这个文本（含两句话）的独热码。

* 第一句：[1,1,1,1,1,1,1,1,0,0,0,0,0,0]
* 第二句：[1,1,1,0,0,0,0,1,1,1,1,1,1,1]



#### TF-IDF

TF-IDF (Term Frequency-Inverse Document Frequency) 是一种用于信息检索和文本挖掘的常用加权技术。它用以评估一个词对于一个文件集或一个语料库中的其中一份文件的重要程度。其数值通常会被用来作为文件搜索的相关性排名的一个指标。
TF-IDF 由两部分组成：

* **Term Frequency (TF)**: 词频，表示词在文档中出现的频率。
  数学公式如下：

$$
(t,d) = \frac{\text{词 } t \text{ 在文档 } d \text{ 中出现的次数}}{\text{文档 } d \text{ 的总词数}}
$$

其中，\(t\) 是某个词，\(d\) 是某个文档。

*  **Inverse Document Frequency (IDF)**: 逆文档频率，表示包含某个词的文档的稀少性。如果一个词在很多文档中都很常见，那么它的IDF值就会很低；反之，如果它只在少数文档中出现，那么它的IDF值就会很高。
  数学公式如下：

$$
F(t) = \log\frac{\text{文档集的总文档数}}{\text{包含词 } t \text{ 的文档数} + 1}
$$

注意：分母加1是为了避免除数为0的情况。

$$

$$

* **TF-IDF** 的计算公式为：

$$
TF\text{-}IDF(t,d) = TF(t,d) \times IDF(t)
$$

这个公式结合了词频和逆文档频率，以评估一个词在特定文档中的重要性。如果一个词在文档中出现的频率很高，并且它在整个文档集中很少出现，那么它的TF-IDF值就会很高，表示这个词对于该文档来说很重要。

现在我们回到之前那个例子。

1. 和one-hot一样
2. 和one-hot一样，得到词汇表
   
   |小|美|女|太|可|爱|了|我|要|看|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
   
   
3. 计算词频(TF)值.
   首先统计字典中每个字在句子中出现的频率：
   
   |小|美|女|太|可|爱|了|我|要|看|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
| 2 | 2 | 2 | 1 | 2 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |
| 1 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |
   
   为了防止数值过大，将其归一化，得到下表：
   
   |小|美|女|太|可|爱|了|我|要|看|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
| 0.14 | 0.14 | 0.14 | 0.07 | 0.14 | 0.07 | 0.07 | 0.07 | 0 | 0 | 0 | 0 | 0 | 0 |
| 0.07 | 0.07 | 0.07 | 0 | 0 | 0 | 0 | 0.07 | 0.07 | 0.07 | 0.07 | 0.07 | 0.07 | 0.07 |
   
   
4. 计算逆文档频率(IDF)值
   
   |小|美|女|太|可|爱|了|我|要|看|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
| -0.41 | -0.41 | -0.41 | 0 | 0 | 0 | 0 | -0.41 | 0 | 0 | 0 | 0 | 0 | 0 |
   
   
5. 计算TF-IDF值
   
   |小|美|女|太|可|爱|了|我|要|看|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
| -0.05 | -0.05 | -0.05 | 0 | 0 | 0 | 0 | -0.02 | 0 | 0 | 0 | 0 | 0 | 0 |
| -0.02 | -0.02 | -0.02 | 0 | 0 | 0 | 0 | -0.02 | 0 | 0 | 0 | 0 | 0 | 0 |
   
   

TF-IDF的思想比较简单，却很实用。然而，这种方法还是存在数据稀疏的问题，也没有考虑字的前后信息。

#### n元语法(n-gram)

`n-gram` 是一种在自然语言处理（NLP）和信息检索中常见的概念，用于表示连续的文本或词序列。在`n-gram`中，`n`代表序列中元素的数量。

具体来说，`n-gram`是从一个文本或句子中连续抽取的`n`个词或字符的序列。例如：

* **1-gram (unigram)**: 单独的词或字符。例如，“我”、“是”、“学生”。
* **2-gram (bigram)**: 连续的两个词或字符。例如，“我是”、“是学”、“学生”。
* **3-gram (trigram)**: 连续的三个词或字符。例如，“我是学生”。

以此类推，还有4-gram、5-gram等，但通常随着`n`的增大，序列在文本中出现的频率会迅速降低，因此在实际应用中，3-gram或4-gram是较为常见的选择。

现在，我们用2-gram把上面的例子做一遍，即把下面这两句话：

* 第一句：**小美女太可爱了，我爱小美女。**
* 第二句：**我要看小美女的演唱会。**

分解为2-gram词汇表，去重后得到28个元素的词汇表。

|小|小美|美|美女|女|女太|太|太可|可|可爱|爱|爱了|了|了我|我|我爱|爱小|我要|要|要看|看小|女的|的|的演|演|演唱|唱|唱会|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |-- | -- | -- | -- | -- | -- | -- |

如果结合One-hot，向量化表示这两句话
第一句：[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
第二句：[1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,1,1,1,1,1,1,1,1,]

当然，也可以用TF-IDF编码，向量化文本。

需要注意的是，虽然`n-gram`模型简单且易于实现，但它也存在一些局限性，例如数据稀疏性（对于某些不常见的`n-gram`，其统计信息可能不足）和缺乏上下文信息（`n-gram`只考虑了词或字符的局部顺序，而没有考虑整个句子的上下文）。

#### 词袋模型工作流程

结合上面的例子，我们得到BoW模型的工作流程，如下：

![WordPresentation_Workflow.svg](../images/WordPresentation_Workflow.svg)



## 参考文献

1. [文本离散表示（一）：词袋模型](https://blog.csdn.net/weixin_30767835/article/details/96110823?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-96110823-blog-122502681.235%5Ev43%5Epc_blog_bottom_relevance_base8&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-96110823-blog-122502681.235%5Ev43%5Epc_blog_bottom_relevance_base8&utm_relevant_index=1)

