# 词表达考古史

## 提出问题

通用人工智能（AGI）的终极目标就是让计算机像人一样思考和说话。即让计算机能理解人类的语言，并且用人类的语言和人交流，和帮助人类处理各种事务。因此，我们提出了要解决的问题：

> 首当其冲要解决的问题，就是让计算机理解人类的语言

词表达（Word Presentation）就是指将自然语言中的词转换为机器可理解的形式或含义的过程。

## 解决问题

### 人类的语言

人类的语言有很多种，我们这里只讨论能表示成文字的那些语言。大致分为两个大类：**表音**和**表意**两类。
我们以英语（表音文字）和汉语（表意文字）为例,

* 英语的文本表示：字母 --> 单词 --> （词组） --> 句子
* 汉语的文本表示：字 --> （词语）--> 句子

文本的划分单元从小到大依次为单词、短语、句子、段落和篇章。其中，句子的表示法依赖于单词，而段落又依赖于句子，以此类推，可以说英文文本的表示是建立在单词的表示基础之上的。所以**单词的表示法**最为基础，也最为重要。相应的，中文中最重要的就是**汉字的表示法**。


### 计算机能处理的语言

众所周知，计算机只能处理0和1，即高低电平（两种状态）。所有的文本信息必须转化成数值型数据，才能被计算机处理。对于AI模型来说，非数值型的文本数据不能直接输入机器学习模型，要先经过编码转化成数值型数据才可用于模型训练或预测。而文本表示，就是研究如何将文本数据合理编码成数值型数据的技术。

### 计算机是如何表示文本

#### 编码表

理科研究，有个共性的办法。找到最小颗粒度的问题，然后研究透彻，然后在组合出复杂的问题。比如物理学，需要研究问题基本粒子的问题。比如生物学，我们需要研究基因。
计算机也不例外，以英文为例，我们先找到最小的单元：**字母**； 然后，把这个转化为0和1。这个过程，专业术语叫**编码**。
鼎鼎大名的[ASCII Table](https://www.ascii-code.com/)就是把字母，数字和特殊字符合在一起的编码表。比如：序号为97的字符a，对应的编码就是01100001。相应的，中文里也有类似的情况，比如GB2312编码表。随着计算机技术的普及，人们就有了统一各种语言的需求，于是乎，大家把各种主流语言的编码表做了一个统一，诞生了UTF-8这种字符表。

Tips:大家学习一下这种典型的计算机邻域解决问题的思路，当每个解决方案只能解决局部问题的时候，我们往往在这些解决方案上面加一层，用统一的办法来解决一类问题。软件领域，有个设计模式叫Adapter，就是这个思想。多层的网络协议栈，也是这种思路。

#### 词的表示方法

在自然语言中，文章是由段落构成，段落由句子组成，而句子由单词构成。然而，每个单词都有多种含义，因此在一片文章里，只看单词很难确定其所表达的含义。只有当单词放到句子中，结合上下文，才能确定单词所表达的含义，进而组合成句子，从而表达句子的语义。
基于这个原因，词表达不仅仅要把词本身用0，1表示出来，还需要结合上下文，精准的表示出其中的语义。
主流的词表达分为两类:

1. 词袋模型(Bag-of-words Model)
2. 词向量法(Word Embedding)

2024年，我们都采用就是词向量法。本文的重心也将放在介绍词向量法。现在，还有一个问题亟待解决。这个问题就是词(word)本身的定义。以中英文为例，如何让计算机统一处理英语单词（Word）和汉字？

#### 分词（Tokenizer）

如何把各种人类的语言变成统一格式，交给AI处理呢？我们先在计算机范畴内，找找看有没有类似的问题。
有了，计算机需要处理很多种编程语言，如C/Java/Go/Javascript等。和我们NLP里，遇到这个问题类似。我们一起来重温一下编译原理的处理过程。

![WordPresentation_Compile.svg](../images/WordPresentation_Compile.svg)

从上图可以看出，编译原理的第一步，词法分析就做了这个事情，把编程语言转化成Token，交给编译器处理。这个过程就叫分词（Tokenizer）。
同样的，在NLP种，我们也参考编译原理的处理方法。把各种人类的语言，转化为Token。交给AI来统一处理。

Tips: Token没有特别的中文翻译，后续文章提到词，指的就是统一格式的Token。

### 词袋模型(BoW:Bag-of-Words)

词袋模型是一种在自然语言处理（NLP）和信息检索（IR）中广泛使用的简化表达模型。举个例子，假如现在有1,000篇新闻文档，把这些文档拆成一个个的字，去重后得到3,000个字，然后把这3,000个字作为字典，进行文本表示的模型，叫做词袋模型。这种模型的特点是字典中的字没有特定的顺序，句子的总体结构也被舍弃了。在本章节里，我们将介绍三种最常用的，分别是one-hot（独热码），TF-IDF和n-gram。

这里，我们用一个简单的例子来讲解这三种方式。一个文本有两句话：

* 第一句：**小美女太可爱了，我爱小美女。**
* 第二句：**我要看小美女的演唱会。**

#### One-hot

针对上面提到的例子，One-hot是这样做的。

1. 先把这两句话拆成一个个字，一共有22个字。
   
   |小|美|女|太|可|爱|了|我|爱|小|美|女|我|要|看|小|美|女|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
   
   
2. 去重，整理得到不重复的字，一共有14个字。
   
   |小|美|女|太|可|爱|了|我|要|看|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
   
   
3. 统计第一句话中每个字在字典中出现的位置
   
   |小|美|女|太|可|爱|了|我|要|看|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
| 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |
   
   
4. 统计第二句话中每个字在字典中出现的位置，得到词汇频率表(Vocabulary Frequency Table)
   
   |小|美|女|太|可|爱|了|我|要|看|的|演|唱|会|
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
| 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |
| 1 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |
   
   
5. 向量化表示这两句话，得到这个文本（含两句话）的独热码。

* 第一句：[1,1,1,1,1,1,1,1,0,0,0,0,0,0]
* 第一句：[1,1,1,0,0,0,0,1,1,1,1,1,1,1]



#### TF-IDF

#### n-gram

## 参考文献

1. [文本离散表示（一）：词袋模型](https://blog.csdn.net/weixin_30767835/article/details/96110823?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-96110823-blog-122502681.235%5Ev43%5Epc_blog_bottom_relevance_base8&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-96110823-blog-122502681.235%5Ev43%5Epc_blog_bottom_relevance_base8&utm_relevant_index=1)

